{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citation Search\n",
    "```Code by Prof. Karl Scheffler```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading graph data and printing first 50 lines\n",
      "\n",
      "629814 entries\n",
      "#*Automated Deduction in Geometry: 5th International Workshop, ADG 2004, Gainesville, FL, USA, September 16-18, 2004, Revised Papers (Lecture Notes in Computer ... / Lecture Notes in Artificial Intelligence)\n",
      "#@Hoon Hong,Dongming Wang\n",
      "#t2006\n",
      "#c\n",
      "#index0\n",
      "\n",
      "#*A+ Certification Core Hardware (Text & Lab Manual)\n",
      "#@Charles J. Brooks\n",
      "#t2003\n",
      "#c\n",
      "#index1\n",
      "\n",
      "#*Performance engineering in industry: current practices and adoption challenges\n",
      "#@Ahmed E. Hassan,Parminder Flora\n",
      "#t2007\n",
      "#cProceedings of the 6th international workshop on Software and performance\n",
      "#index2\n",
      "#!This panel session discusses performance engineering practices in industry. Presentations in the session will explore the use of lightweight techniques and approaches in order to permit the cost effective and rapid adoption of performance modeling research by large industrial software systems.\n",
      "\n",
      "#*Dude, You Can Do It! How to Build a Sweeet PC\n",
      "#@Darrel Creacy,Carlito Vicencio\n",
      "#t2005\n",
      "#c\n",
      "#index3\n",
      "#!Whether you're frustrated with current PC offerings (and their inflated prices) or are simply looking for a cool project to take on, building a computer from the ground up using off-the-shelf parts can offer significant advantages. In these pages, computer dudes Darrel Wayne Creacy and Carlito Vicencio outline those advantages and then show you how to build the computer of your dreams. The pair begins by explaining what components make up a PC and what you need to think about when selecting those components, before helping you determine your needs and suggesting various configurations to fit those uses. Breaking the process down into its simplest terms, the authors provide component lists for a number of different PC setups: for students, home users, multimedia/home-theater enthusiasts, high-end graphic/video/audio producers, and more. Using plain language and plenty of visual and instructional aids--photos, illustrations, diagrams, step-by-step directions, and more--the authors ensure that even someone (like you!) who knows nothing about technology can build the perfect PC! On a more personal note, the authors are donating a percentage of their income from this book to the Breast Cancer Research Foundationï¾to thank all the women in their lives who have supported them and battled the disease. For more information about BCRF, please visit http://www.bcrfcure.org/.\n",
      "\n",
      "#*What Every Programmer Needs to Know about Security (Advances in Information Security)\n",
      "#@Neil Daswani,Anita Kesavan\n",
      "#t2006\n",
      "#c\n",
      "#index4\n",
      "\n",
      "#*Interpreting Kullback-Leibler divergence with the Neyman-Pearson lemma\n",
      "#@Shinto Eguchi,John Copas\n",
      "#t2006\n",
      "#cJournal of Multivariate Analysis\n",
      "#index5\n",
      "#%436405\n",
      "#!Kullback-Leibler divergence and the Neyman-Pearson lemma are two fundamental concepts in statistics. Both are about likelihood ratios: Kullback-Leibler divergence is the expected log-likelihood ratio, and the Neyman-Pearson lemma is about error rates of likelihood ratio tests. Exploring this connection gives another statistical interpretation of the Kullback-Leibler divergence in terms of the loss of power of the likelihood ratio test when the wrong distribution is used for one of the hypotheses. In this interpretation, the standard non-negativity property of the Kullback-Leibler divergence is essentially a restatement of the optimal property of likelihood ratios established by the Neyman-Pearson lemma. The asymmetry of Kullback-Leibler divergence is overviewed in information geometry.\n",
      "\n",
      "#*Digital Media: Transformations in Human Communication\n",
      "#@Lee Humphreys,Paul Messaris\n",
      "#t2006\n",
      "#c\n",
      "#index6\n",
      "\n",
      "#*TOPP---the OpenMS proteomics pipeline\n",
      "#@Oliver Kohlbacher,Knut Reinert,Clemens Gröpl,Eva Lange,Nico Pfeifer,Ole Schulz-Trieglaff,Marc Sturm\n",
      "#t2007\n",
      "#cBioinformatics\n",
      "\n",
      "----------\n",
      "Done!\n",
      "The citation network has 629814 nodes and 632751 edges\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This cell loads all data and builds the citation graph.\n",
    "\n",
    " * Most of the code below is for\n",
    "\n",
    "    * getting a zip file from an online source,\n",
    "    * unzipping it, and\n",
    "    * processing the resulting text file format to get the data we want.\n",
    "\n",
    "   You don't have to worry about the details of how this works.\n",
    "\n",
    " * The result is a directed NetworkX graph with an edge from node A to\n",
    "   node B meaning that node A cites node B as a reference.\n",
    "\n",
    " * We also construct a dictionary, mapping the id of an article to the\n",
    "   list of unique words appearing in that article. Capitalization is\n",
    "   removed from all words.\n",
    "'''\n",
    "\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from requests import get\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Data from: https://www.aminer.cn/citation\n",
    "request = get('https://lfs.aminer.cn/lab-datasets/citation/citation-network1.zip')\n",
    "zip_file = ZipFile(BytesIO(request.content))\n",
    "first_lines = 50\n",
    "\n",
    "print('Loading graph data and printing first', first_lines, 'lines\\n')\n",
    "\n",
    "g = nx.DiGraph()\n",
    "article_words = {}\n",
    "with zip_file.open('outputacm.txt', 'r') as fp:\n",
    "    entry_count = int(next(fp).strip())\n",
    "    print(entry_count, 'entries')\n",
    "    for expected_index in range(entry_count):\n",
    "        lines = []\n",
    "        while True:\n",
    "            line = next(fp).decode('utf-8').strip()\n",
    "            if first_lines > 0:\n",
    "                print(line)\n",
    "            first_lines -= 1\n",
    "            if line == '':\n",
    "                break\n",
    "            lines.append(line)\n",
    "        entry = {}\n",
    "        line_counter = 0\n",
    "        for key, prefix, multiple, mapping in [\n",
    "            ('title', '#*', False, str), ('authors', '#@', False, str),\n",
    "            ('year', '#t', False, int), ('venue', '#c', False, str),\n",
    "            ('id', '#index', False, int), ('references', '#%', True, int),\n",
    "            ('abstract', '#!', False, str),\n",
    "        ]:\n",
    "            if multiple:\n",
    "                entry[key] = []\n",
    "            while (\n",
    "                line_counter < len(lines) and\n",
    "                lines[line_counter].startswith(prefix)\n",
    "            ):\n",
    "                value = mapping(lines[line_counter][len(prefix):])\n",
    "                if multiple:\n",
    "                    entry[key].append(value)\n",
    "                else:\n",
    "                    if key in entry:\n",
    "                        import pdb\n",
    "                        pdb.set_trace()\n",
    "                    assert key not in entry\n",
    "                    entry[key] = value\n",
    "                line_counter += 1\n",
    "        g.add_node(entry['id'])\n",
    "        g.nodes[entry['id']]['title'] = entry['title']\n",
    "        for ref_id in entry['references']:\n",
    "            g.add_edge(entry['id'], ref_id)\n",
    "        article_words[entry['id']] = set(\n",
    "            entry.get('abstract', '').strip().lower().split())\n",
    "\n",
    "print()\n",
    "print('----------')\n",
    "print('Done!')\n",
    "print('The citation network has', g.number_of_nodes(), 'nodes and', g.number_of_edges(), 'edges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 807190 unique words\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This cell builds an _inverted index_ by mapping all unique words in the whole\n",
    "data set to the article ids in which they appear.\n",
    "'''\n",
    "word_counts = defaultdict(int)\n",
    "inverted_index = defaultdict(list)\n",
    "for article_id, words in article_words.items():\n",
    "    for word in words:\n",
    "        word_counts[word] += 1\n",
    "        inverted_index[word].append(article_id)\n",
    "\n",
    "print('There are', len(inverted_index), 'unique words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The word human appears in 9254 articles. The first 5 are\n",
      " * Webbots, Spiders, and Screen Scrapers\n",
      " * A hybrid approach for fabrication of polymeric BIOMEMS devices\n",
      " * Metrics for evaluating human information interaction systems\n",
      " * Implicit Meshes for Effective Silhouette Handling\n",
      " * Rapid and brief communication: An algorithm for semi-supervised learning in image retrieval\n",
      "\n",
      "The word computer appears in 23774 articles. The first 5 are\n",
      " * Dude, You Can Do It! How to Build a Sweeet PC\n",
      " * A control word model for detecting conflicts between microoperations\n",
      " * Design team composition for high level language computer architectures\n",
      " * CompTIA A+ Exam Cram (Exams 220-602, 220-603, 220-604) (Exam Cram)\n",
      " * The Advantage Series: Microsoft Office Word 2003, Brief Edition, 1 edition\n",
      "\n",
      "The word artificial appears in 4277 articles. The first 5 are\n",
      " * Controlling simulation games through rule-based scenarios\n",
      " * Game Programming for Teens (Game Programming)\n",
      " * Emotion representation and physiology assignments in digital systems\n",
      " * Image languages in intelligent radiological palm diagnostics\n",
      " * Parallel Adaptive Estimation of Hip Range of Motion for Total Hip Replacement Surgery*A preliminary version of this paper was presented at the 7th Int'l Conf. Medical Image Computing and Computer-Assisted Intervention (MICCAI 2004).\n"
     ]
    }
   ],
   "source": [
    "for word in ['human', 'computer', 'artificial']:\n",
    "    print()\n",
    "    print('The word', word, 'appears in', len(inverted_index[word]), 'articles. The first 5 are')\n",
    "    for article_id in inverted_index[word][:5]:\n",
    "        print(' *', g.nodes[article_id]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " * The search function below uses the inverted index to find the articles\n",
    "   containing all keywords.\n",
    "\n",
    " * It then sorts the articles using the PageRank algorithm.\n",
    "'''\n",
    "\n",
    "def search(phrase):\n",
    "    print('Searching for %r' % phrase)\n",
    "\n",
    "    # Find abstracts containing all keywords\n",
    "    keywords = phrase.strip().lower().split()\n",
    "    result_set = set(inverted_index[keywords[0]])\n",
    "    for word in keywords[1:]:\n",
    "        result_set.intersection_update(inverted_index[word])\n",
    "\n",
    "    # Map all article ids to the same weight (1) for PageRank\n",
    "    result_set = dict.fromkeys(result_set, 1)\n",
    "    print('Results (%i hits):' % len(result_set))\n",
    "\n",
    "    # Compute PageRank using results, sorted from highest to lowest score\n",
    "    pagerank = sorted(\n",
    "        nx.pagerank_scipy(g, personalization=result_set, max_iter=30).items(),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True)\n",
    "\n",
    "    # Print results\n",
    "    top_score = pagerank[0][1]\n",
    "    for i in range(10):\n",
    "        print('%2i. %3.0f%% - %s' % (i + 1, 100 * pagerank[i][1] / top_score, g.nodes[pagerank[i][0]]['title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for 'Monte Carlo'\n",
      "Results (1181 hits):\n",
      " 1. 100% - Simulation Modeling and Analysis, 2nd edition\n",
      " 2.  84% - Random number generation and quasi-Monte Carlo methods\n",
      " 3.  82% - Stochastic finite elements: a spectral approach\n",
      " 4.  74% - Monte Carlo Statistical Methods (Springer Texts in Statistics)\n",
      " 5.  62% - Statistical Timing Analysis Considering Spatial Correlations using a Single Pert-Like Traversal\n",
      " 6.  56% - Monte Carlo methods. Vol. 1: basics\n",
      " 7.  52% - Time series: theory and methods\n",
      " 8.  48% - Statistical analysis with missing data\n",
      " 9.  48% - Large sample properties of simulations using latin hypercube sampling\n",
      "10.  47% - Simulation Modeling and Analysis, 3rd edition\n"
     ]
    }
   ],
   "source": [
    "search('Monte Carlo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for 'Bayesian Inference'\n",
      "Results (263 hits):\n",
      " 1. 100% - Probabilistic reasoning in intelligent systems: networks of plausible inference\n",
      " 2.  29% - An Introduction to Variational Methods for Graphical Models\n",
      " 3.  28% - Information Theory, Inference & Learning Algorithms\n",
      " 4.  25% - Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)\n",
      " 5.  22% - mStruct: a new admixture model for inference of population structure in light of both genetic admixing and allele mutations\n",
      " 6.  22% - A general approach to heteroscedastic linear regression\n",
      " 7.  21% - Transformation and weighting in regression\n",
      " 8.  18% - Assessing Approximate Inference for Binary Gaussian Process Classification\n",
      " 9.  17% - Naive Bayes models for probability estimation\n",
      "10.  17% - Bayesian Inference and Optimal Design for the Sparse Linear Model\n"
     ]
    }
   ],
   "source": [
    "search('Bayesian Inference')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
